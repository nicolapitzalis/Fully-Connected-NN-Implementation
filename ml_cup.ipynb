{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.neural_network import NeuralNetwork\n",
    "from network.neural_network_utility import evaluate\n",
    "from math_functions.function_enums import LossFunction, ActivationFunction, Metrics\n",
    "from utilities.dataset_reader import read_cup, read_cup_ext_test\n",
    "from model_selection.validation import kfold_cv_ensemble\n",
    "from model_selection.grid import grid_search, get_top_n_results, get_all_results\n",
    "from utilities.utils import count_configs, get_list_models, plot_over_epochs, save_array_with_comments\n",
    "from network.ensemble import Ensemble\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path for development set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SEL_DATA_PATH = 'datasets/cup/grid_search/CUP_model_selection_data.csv'\n",
    "MODEL_SEL_TARGETS_PATH = 'datasets/cup/grid_search/CUP_model_selection_targets.csv'\n",
    "MODEL_ASSESS_DATA_PATH = 'datasets/cup/grid_search/CUP_model_assessment_data.csv'\n",
    "MODEL_ASSESS_TARGETS_PATH = 'datasets/cup/grid_search/CUP_model_assessment_targets.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training data and internal test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, targets = read_cup('datasets/cup/CUP_TR.csv')\n",
    "# train_data, test_data, train_targets, test_targets = holdout(data, targets, 0.8, shuffle_set=True)\n",
    "\n",
    "# np.savetxt(MODEL_SEL_DATA_PATH, train_data, delimiter=',')\n",
    "# np.savetxt(MODEL_SEL_TARGETS_PATH, train_targets, delimiter=',')\n",
    "# np.savetxt(MODEL_ASSESS_DATA_PATH, test_data, delimiter=',')\n",
    "# np.savetxt(MODEL_ASSESS_TARGETS_PATH, test_targets, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and internal test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(MODEL_SEL_DATA_PATH, delimiter=',')\n",
    "train_targets = np.loadtxt(MODEL_SEL_TARGETS_PATH, delimiter=',')\n",
    "test_data = np.loadtxt(MODEL_ASSESS_DATA_PATH, delimiter=',')\n",
    "test_targets = np.loadtxt(MODEL_ASSESS_TARGETS_PATH, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    classification=False, \n",
    "    early_stopping=True, \n",
    "    fast_stopping=False,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    epochs=500,\n",
    "    linear_decay=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "grid_params = dict(\n",
    "    batch_size=[1, 64, 128],\n",
    "    hidden_layer_sizes=[[64, 32], [128, 64], [128, 128], [64, 64, 32]],\n",
    "    learning_rate=[0.0005, 0.005, 0.05, 0.1],\n",
    "    mom_alpha=[0, 0.6, 0.7, 0.9],\n",
    "    reg_lambda=[0, 0.0001, 0.00001],\n",
    "    nesterov=[True, False],\n",
    "    tao=[200, 500]\n",
    ")\n",
    "\n",
    "results = grid_search(\n",
    "    k_folds=3, \n",
    "    data=train_data, \n",
    "    target=train_targets, \n",
    "    metrics=[Metrics.MSE.value, Metrics.MEE.value], \n",
    "    fixed_param=fixed_params, \n",
    "    grid_param=grid_params, \n",
    "    file_name_results=\"ml_cup_first_grid_full\", \n",
    "    verbose=False,\n",
    "    plot=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_merged = get_top_n_results('ml_cup_first_grid_full.json', 100, 'validation_mee_mean', ascending=True)\n",
    "top_50_merged = get_top_n_results('ml_cup_first_grid_full.json', 50, 'validation_mee_mean', ascending=True)\n",
    "top_20_merged = get_top_n_results('ml_cup_first_grid_full.json', 20, 'validation_mee_mean', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_all_results('ml_cup_first_grid_full.json')\n",
    "print(f'total number of explored configs: {len(results)}')\n",
    "\n",
    "validation_mee_means = {model[0]: model[1]['validation_mee_mean'] for model in top_100_merged}\n",
    "\n",
    "for model_config, mee_mean in validation_mee_means.items():\n",
    "    print(f\"{model_config}: {mee_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOP-100----------------')\n",
    "count_configs(top_100_merged)\n",
    "print('\\nTOP-50----------------')\n",
    "count_configs(top_50_merged)\n",
    "print('\\nTOP-20----------------')\n",
    "count_configs(top_20_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second grid on batch size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size 64 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    classification=False, \n",
    "    early_stopping=True, \n",
    "    fast_stopping=False,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    epochs=2000,\n",
    "    linear_decay=True,\n",
    "    batch_size=64,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    nesterov=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "grid_params = dict(\n",
    "    learning_rate=[0.02, 0.04, 0.05, 0.08, 0.1, 0.2],\n",
    "    mom_alpha=[0, 0.6, 0.7, 0.9],\n",
    "    reg_lambda=[0, 0.00001],\n",
    "    tao=[500, 1000]\n",
    ")\n",
    "\n",
    "results = grid_search(\n",
    "    k_folds=5, \n",
    "    data=train_data, \n",
    "    target=train_targets, \n",
    "    metrics=[Metrics.MSE.value, Metrics.MEE.value], \n",
    "    fixed_param=fixed_params, \n",
    "    grid_param=grid_params, \n",
    "    file_name_results=\"ml_cup_second_grid_batch_64\", \n",
    "    verbose=False,\n",
    "    plot=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second grid on batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size 1 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    classification=False, \n",
    "    early_stopping=True, \n",
    "    fast_stopping=False,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    epochs=2000,\n",
    "    linear_decay=True,\n",
    "    batch_size=1,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    nesterov=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "grid_params = dict(\n",
    "    learning_rate=[0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008],\n",
    "    mom_alpha=[0.5, 0.6, 0.7],\n",
    "    reg_lambda=[0, 0.0001, 0.00001, 0.00005],\n",
    "    tao=[500, 1000]\n",
    ")\n",
    "\n",
    "results = grid_search(\n",
    "    k_folds=5, \n",
    "    data=train_data, \n",
    "    target=train_targets, \n",
    "    metrics=[Metrics.MSE.value, Metrics.MEE.value], \n",
    "    fixed_param=fixed_params, \n",
    "    grid_param=grid_params, \n",
    "    file_name_results=\"ml_cup_second_grid_batch_1\", \n",
    "    verbose=False,\n",
    "    plot=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_second_batch_64 = get_top_n_results('ml_cup_second_grid_batch_64.json', 100, 'validation_mee_mean', ascending=True)\n",
    "top_50_second_batch_64 = get_top_n_results('ml_cup_second_grid_batch_64.json', 50, 'validation_mee_mean', ascending=True)\n",
    "top_20_second_batch_64 = get_top_n_results('ml_cup_second_grid_batch_64.json', 20, 'validation_mee_mean', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_all_results('ml_cup_second_grid_batch_64.json')\n",
    "print(f'total number of explored configs: {len(results)}')\n",
    "\n",
    "validation_mee_means = {model[0]: model[1]['validation_mee_mean'] for model in top_100_second_batch_64}\n",
    "\n",
    "for model_config, mee_mean in validation_mee_means.items():\n",
    "    print(f\"{model_config}: {mee_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_second_batch_1 = get_top_n_results('ml_cup_second_grid_batch_1.json', 100, 'validation_mee_mean', ascending=True)\n",
    "top_50_second_batch_1 = get_top_n_results('ml_cup_second_grid_batch_1.json', 50, 'validation_mee_mean', ascending=True)\n",
    "top_20_second_batch_1 = get_top_n_results('ml_cup_second_grid_batch_1.json', 20, 'validation_mee_mean', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_all_results('ml_cup_second_grid_batch_1.json')\n",
    "print(f'total number of explored configs: {len(results)}')\n",
    "\n",
    "validation_mee_means = {model[0]: model[1]['validation_mee_mean'] for model in top_100_second_batch_1}\n",
    "\n",
    "for model_config, mee_mean in validation_mee_means.items():\n",
    "    print(f\"{model_config}: {mee_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on the top model for the 64 batch_size version\n",
    "\n",
    "we plot on the internal test set (no operation is done with the internal test set, it's just a plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    classification=False, \n",
    "    early_stopping=False, \n",
    "    fast_stopping=False,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    epochs=2000,\n",
    "    linear_decay=True,\n",
    "    batch_size=1,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    nesterov=True,\n",
    "    learning_rate=0.006,\n",
    "    mom_alpha=0.6,\n",
    "    reg_lambda=0.00005,\n",
    "    tao=1000,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "net.train_net(train_data=train_data, train_target=train_targets, val_data=test_data, val_target=test_targets, tr_loss_stopping_point=0.07113932071043247)\n",
    "net.predict_and_evaluate(test_data, test_targets, Metrics.MEE.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_epochs(y_values=net.training_losses, title='loss over epochs', y_label='mse', y_legend='training set', y_prime_values=net.validation_losses, y_prime_legend='test set', yscale='log')\n",
    "plot_over_epochs(y_values=net.training_evaluations, title='score over epochs', y_label='mee', y_legend='training set', y_prime_values=net.validation_evaluations, y_prime_legend='test set', yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "After picking the best 10 performing models, we decided to build an ensamble in order to improve performance.\n",
    "\n",
    "Note: we won't train the models using early stopping right now. We will use more data to train the models and use the training loss mean of the best epoch (w.r.t. the internal valiation loss) obtained during the kfold cv in the grid process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 results from both batches grids\n",
    "top_10_second_batch_1 = get_top_n_results('ml_cup_second_grid_batch_1.json', 10, 'validation_mee_mean', ascending=True)\n",
    "top_10_second_batch_64 = get_top_n_results('ml_cup_second_grid_batch_64.json', 10, 'validation_mee_mean', ascending=True)\n",
    "\n",
    "# merge them together and sort by validation_mee_mean\n",
    "merged_data = top_10_second_batch_1 + top_10_second_batch_64\n",
    "sorted_data = sorted(merged_data, key=lambda x: x[1]['validation_mee_mean'])\n",
    "\n",
    "# get the final top 10 results\n",
    "top_10_models_results = sorted_data[:10]\n",
    "\n",
    "# get the stopping point for training the retrained model over the entire development dataset\n",
    "tr_stopping_points = [results[1]['tr_losses_mean'] for results in top_10_models_results]\n",
    "\n",
    "# get the top 10 models configurations\n",
    "list_models = get_list_models(top_10_models_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we execute a kfold over the ensemble (and over each constituent model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    epochs=2000, \n",
    "    nesterov=False, \n",
    "    classification=False, \n",
    "    early_stopping=False, \n",
    "    fast_stopping=False,\n",
    "    linear_decay=True,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# build ensemble\n",
    "models = []\n",
    "for index, model in enumerate(list_models):\n",
    "    params = {**fixed_params, **model}\n",
    "    models.append(NeuralNetwork(**fixed_params, **model))\n",
    "ensemble = Ensemble(models)\n",
    "\n",
    "# cross-validation on ensemble\n",
    "kfold_model_result, kfold_ensemble_result = kfold_cv_ensemble(10, train_data, train_targets, [Metrics.MSE.value, Metrics.MEE.value], ensemble, tr_stopping_points=tr_stopping_points, verbose=False)\n",
    "\n",
    "with open('json_results/ensemble_top_10_tao_2000.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({**kfold_model_result, **kfold_ensemble_result}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on the kfold cross validation has shown that the best performing model is indeed the ensemble of the top 10 models. Thus we now retrain the model on the entire internal training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    epochs=2000, \n",
    "    nesterov=False, \n",
    "    classification=False, \n",
    "    early_stopping=False, \n",
    "    fast_stopping=False,\n",
    "    linear_decay=True,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# build ensemble\n",
    "models = []\n",
    "for index, model in enumerate(list_models):\n",
    "    params = {**fixed_params, **model}\n",
    "    models.append(NeuralNetwork(**fixed_params, **model))\n",
    "ensemble = Ensemble(models)\n",
    "\n",
    "# train ensemble on the entire development dataset\n",
    "ensemble.train(train_data, train_targets, test_data, test_targets, tr_stopping_points=tr_stopping_points)\n",
    "\n",
    "# evaluate ensemble on the test set and save results\n",
    "results = {}\n",
    "for index, model in enumerate(ensemble.models):\n",
    "    y_pred = model.predict(test_data)\n",
    "    results[f\"model_{index+1}\"] = evaluate(y_pred, test_targets, Metrics.MEE.value)\n",
    "    \n",
    "y_pred = ensemble.predict(test_data)\n",
    "results[\"ensemble\"] = evaluate(y_pred, test_targets, Metrics.MEE.value)\n",
    "\n",
    "with open('json_results/ensemble_top_10_test_results_tao_2000.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting curves for the top-10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_losses = [model.training_losses for model in ensemble.models]\n",
    "all_validation_losses = [model.validation_losses for model in ensemble.models]\n",
    "max_length_tr_losses = max(len(loss_array) for loss_array in all_training_losses)\n",
    "max_length_val_losses = max(len(loss_array) for loss_array in all_validation_losses)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, model in enumerate(ensemble.models):\n",
    "\n",
    "    # Pad shorter arrays\n",
    "    padded_data_tr_losses = [np.pad(loss_array, (0, max_length_tr_losses - len(loss_array)), 'edge') for loss_array in all_training_losses]\n",
    "    padded_data_val_losses = [np.pad(loss_array, (0, max_length_val_losses - len(loss_array)), 'edge') for loss_array in all_validation_losses]\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(list(range(len(model.training_losses))), model.training_losses, color='blue', alpha=0.2, label='Training set')\n",
    "        plt.plot(list(range(len(model.validation_losses))), model.validation_losses, color='red', alpha=0.2, label='Internal test set')\n",
    "    else: \n",
    "        plt.plot(list(range(len(model.training_losses))), model.training_losses, color='blue', alpha=0.2)\n",
    "        plt.plot(list(range(len(model.validation_losses))), model.validation_losses, color='red', alpha=0.2)\n",
    "        \n",
    "\n",
    "plt.plot(np.mean(padded_data_tr_losses, axis=0), label=\"Mean training set\", linestyle='--', color='darkblue')\n",
    "plt.plot(np.mean(padded_data_val_losses, axis=0), label=\"Mean internal test set\", linestyle='--', color='darkred')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_evals = [model.training_evaluations for model in ensemble.models]\n",
    "all_validation_evals = [model.validation_evaluations for model in ensemble.models]\n",
    "max_length_tr_evals = max(len(loss_array) for loss_array in all_training_evals)\n",
    "max_length_val_evals = max(len(loss_array) for loss_array in all_validation_evals)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, model in enumerate(ensemble.models):\n",
    "\n",
    "    # Pad shorter arrays\n",
    "    padded_data_tr_evals = [np.pad(loss_array, (0, max_length_tr_evals - len(loss_array)), 'edge') for loss_array in all_training_evals]\n",
    "    padded_data_val_evals = [np.pad(loss_array, (0, max_length_val_evals - len(loss_array)), 'edge') for loss_array in all_validation_evals]\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(list(range(len(model.training_losses))), model.training_evaluations, color='blue', alpha=0.2, label='Training set')\n",
    "        plt.plot(list(range(len(model.validation_losses))), model.validation_evaluations, color='red', alpha=0.2, label='Internal test set')\n",
    "    else: \n",
    "        plt.plot(list(range(len(model.training_losses))), model.training_evaluations, color='blue', alpha=0.2)\n",
    "        plt.plot(list(range(len(model.validation_losses))), model.validation_evaluations, color='red', alpha=0.2)\n",
    "        \n",
    "\n",
    "plt.plot(np.mean(padded_data_tr_evals, axis=0), label=\"Mean training set\", linestyle='--', color='darkblue')\n",
    "plt.plot(np.mean(padded_data_val_evals, axis=0), label=\"Mean internal test set\", linestyle='--', color='darkred')\n",
    "plt.yscale('log')\n",
    "plt.title('Score over epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mee')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_losses = [model.training_losses[:500] for model in ensemble.models]\n",
    "all_validation_losses = [model.validation_losses[:500] for model in ensemble.models]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, model in enumerate(ensemble.models):\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(list(range(500)), model.training_losses[:500], color='blue', alpha=0.2, label='Training set')\n",
    "        plt.plot(list(range(500)), model.validation_losses[:500], color='red', alpha=0.2, label='Internal test set')\n",
    "    else: \n",
    "        plt.plot(list(range(500)), model.training_losses[:500], color='blue', alpha=0.2)\n",
    "        plt.plot(list(range(500)), model.validation_losses[:500], color='red', alpha=0.2)\n",
    "        \n",
    "\n",
    "plt.plot(np.mean(all_training_losses, axis=0), label=\"Mean training set\", linestyle='--', color='darkblue')\n",
    "plt.plot(np.mean(all_validation_losses, axis=0), label=\"Mean internal test set\", linestyle='--', color='darkred')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_evaluations = [model.training_evaluations[:500] for model in ensemble.models]\n",
    "all_validation_evaluations = [model.validation_evaluations[:500] for model in ensemble.models]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, model in enumerate(ensemble.models):\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(list(range(500)), model.training_evaluations[:500], color='blue', alpha=0.2, label='Training set')\n",
    "        plt.plot(list(range(500)), model.validation_evaluations[:500], color='red', alpha=0.2, label='Internal test set')\n",
    "    else: \n",
    "        plt.plot(list(range(500)), model.training_evaluations[:500], color='blue', alpha=0.2)\n",
    "        plt.plot(list(range(500)), model.validation_evaluations[:500], color='red', alpha=0.2)\n",
    "        \n",
    "\n",
    "plt.plot(np.mean(all_training_evaluations, axis=0), label=\"Mean training set\", linestyle='--', color='darkblue')\n",
    "plt.plot(np.mean(all_validation_evaluations, axis=0), label=\"Mean internal test set\", linestyle='--', color='darkred')\n",
    "plt.yscale('log')\n",
    "plt.title('Score over epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mee')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Assessment on the ensemble\n",
    "\n",
    "Now we will proceede with the model assessment by training and testing over the internal test set multiple times, to then average the results (in order to achieve a less biased result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    epochs=2000, \n",
    "    nesterov=False, \n",
    "    classification=False, \n",
    "    early_stopping=False, \n",
    "    fast_stopping=False,\n",
    "    linear_decay=True,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(10):\n",
    "    # build ensemble\n",
    "    models = []\n",
    "    for index, model in enumerate(list_models):\n",
    "        params = {**fixed_params, **model}\n",
    "        models.append(NeuralNetwork(**fixed_params, **model))\n",
    "    ensemble = Ensemble(models)\n",
    "\n",
    "    # train ensemble on the entire development dataset\n",
    "    ensemble.train(train_data, train_targets, test_data, test_targets, tr_stopping_points=tr_stopping_points)\n",
    "\n",
    "        \n",
    "    y_pred = ensemble.predict(test_data)\n",
    "    results.append(evaluate(y_pred, test_targets, Metrics.MEE.value))\n",
    "\n",
    "model_assessment = {}\n",
    "model_assessment['test_results'] = results\n",
    "model_assessment['test_mean'] = np.mean(results)\n",
    "model_assessment['test_std'] = np.std(results)\n",
    "\n",
    "with open('json_results/ensemble_top_10_model_assessment_tao_2000.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_assessment, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining the chosen model on the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we assessed the model performance, we can retrain the model using the entire training set.\n",
    "\n",
    "Note: we use the mean training loss obtained at the best epoch (w.r.t. the validation loss) as stopping criteria for the training. In this way we assure that the same level of fitting is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = read_cup('datasets/cup/CUP_TR.csv')\n",
    "external_test_data = read_cup_ext_test('datasets/cup/CUP_TS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    n_output_units=3, \n",
    "    training_loss_type_value=LossFunction.MSE.value, \n",
    "    validation_loss_type_value=LossFunction.MSE.value, \n",
    "    evaluation_metric_type_value=Metrics.MEE.value,\n",
    "    activation_hidden_type_value=ActivationFunction.SIGMOID.value, \n",
    "    activation_output_type_value=ActivationFunction.IDENTITY.value, \n",
    "    epochs=2000, \n",
    "    nesterov=False, \n",
    "    classification=False, \n",
    "    early_stopping=False, \n",
    "    fast_stopping=False,\n",
    "    linear_decay=True,\n",
    "    patience=20, \n",
    "    tolerance=0.1,\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "models = []\n",
    "for index, model in enumerate(list_models):\n",
    "    params = {**fixed_params, **model}\n",
    "    models.append(NeuralNetwork(**fixed_params, **model))\n",
    "ensemble = Ensemble(models)\n",
    "\n",
    "# train ensemble on the entire development dataset\n",
    "ensemble.train(data, targets, None, None, tr_stopping_points=tr_stopping_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the external test set\n",
    "predict = ensemble.predict(external_test_data)\n",
    "\n",
    "csv_comments = [\"Nicoletta Alice, Piccolo Chiara, Pitzalis Nicola\",\n",
    "                    \"quiquoqua\",\n",
    "                    \"ML-CUP23\",\n",
    "                    \"14/01/2024\"]\n",
    "\n",
    "# Save the example array to a file\n",
    "example_file_path = 'csv_results/quiquoqua_ML-CUP23-TS.csv'\n",
    "save_array_with_comments(predict, example_file_path, csv_comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
